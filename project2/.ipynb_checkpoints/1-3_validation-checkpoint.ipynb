{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "approximate-batman",
   "metadata": {},
   "source": [
    "pip install phik\n",
    "pip install pandas-profiling\n",
    "pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spoken-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import missingno as msgn\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "import statsmodels.api as sm\n",
    "import pylab as py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-evanescence",
   "metadata": {},
   "source": [
    "## PNR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "congressional-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnrs = pd.read_csv('PNR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approximate-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting pnrs that has no flight\n",
    "pnrs.dropna(subset=['ID_PNR_ITI_DEP_YMD_1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "skilled-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "   pnrs['date'] = pnrs['ID_PNR_CREATION_DTIME_GMT'].apply(lambda x: x.split(' ')[0])\n",
    "# pnr creation month\n",
    "pnrs['pnr_create_month'] = pd.to_datetime(pnrs['ID_PNR_CREATION_DTIME_GMT']).dt.month\n",
    "# pnr creation weekday\n",
    "pnrs['pnr_create_weekday'] = pd.to_datetime(pnrs['ID_PNR_CREATION_DTIME_GMT']).dt.weekday\n",
    "\n",
    "pnrs['ID_PNR_ITI_DEP_YMD_1'] = pnrs['ID_PNR_ITI_DEP_YMD_1'].apply(lambda x: datetime.strptime(str(int(x)), '%Y%m%d').strftime('%Y-%m-%d'))\n",
    "# month of first flight\n",
    "pnrs['flight_month_1'] = pd.to_datetime(pnrs['ID_PNR_ITI_DEP_YMD_1']).dt.month\n",
    "# day of first flight\n",
    "pnrs['flight_weekday_1'] =  pd.to_datetime(pnrs['ID_PNR_ITI_DEP_YMD_1']).dt.weekday\n",
    "\n",
    "pnrs['ID_PNR_ITI_DEP_YMD_2'] = pnrs['ID_PNR_ITI_DEP_YMD_2'].apply(lambda x: datetime.strptime(str(int(x)), '%Y%m%d').strftime('%Y-%m-%d') if not pd.isna(x) else np.nan)\n",
    "# month of second flight\n",
    "pnrs['flight_month_2'] = pd.to_datetime(pnrs['ID_PNR_ITI_DEP_YMD_2']).dt.month # apply(lambda x: int(x.month) if not pd.isna(x) else 0)\n",
    "# day of second flight\n",
    "pnrs['flight_weekday_2'] =  pd.to_datetime(pnrs['ID_PNR_ITI_DEP_YMD_2']).dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "colonial-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_hours = lambda x: datetime(1900, 1, 1, int(x)//100, int(x)%100)\n",
    "find_hour_difference = lambda x,y : (convert_hours(x)-convert_hours(y)).total_seconds()/60 if not pd.isna(y) else np.nan\n",
    "def arrival_hour_interval(x):\n",
    "    if 500 <= x < 1100:\n",
    "        return 0\n",
    "    elif 1100 <= x < 1800:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def pnr_workhour_interval(x):\n",
    "    if 800 <= x < 1800:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ultimate-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight duration for first flight\n",
    "pnrs['flight_duration_1'] = pnrs[['ID_PNR_ITI_ARR_TIME_1', 'ID_PNR_ITI_DEP_TIME_1']].apply(lambda x: find_hour_difference(*x), axis=1)\n",
    "pnrs['flight_duration_1'] = pnrs['flight_duration_1'].apply(lambda x: x+1440 if x<0 else x)\n",
    "# flight duration for second flight\n",
    "pnrs['flight_duration_2'] = pnrs[['ID_PNR_ITI_ARR_TIME_2', 'ID_PNR_ITI_DEP_TIME_2']].apply(lambda x: find_hour_difference(*x), axis=1)\n",
    "pnrs['flight_duration_2'] = pnrs['flight_duration_2'].apply(lambda x: x+1440 if x<0 else x)\n",
    "# arrival intervals of flights 05:00-11:00, 11:00-18:00, 18:00-05:00\n",
    "pnrs['arrival_interval_1'] = pnrs['ID_PNR_ITI_ARR_TIME_1'].apply(arrival_hour_interval)\n",
    "pnrs['arrival_interval_2'] = pnrs['ID_PNR_ITI_ARR_TIME_2'].apply(arrival_hour_interval)\n",
    "# day difference between first flight and pnr date\n",
    "pnrs['diff_pnr_firstflight'] = (pd.to_datetime(pnrs['ID_PNR_ITI_DEP_YMD_1'])-pd.to_datetime(pnrs['ID_PNR_CREATION_DTIME_GMT'])).apply(lambda x: x.days) + 1\n",
    "# day difference between second flight and first flight\n",
    "pnrs['diff_second_firstflight'] = np.abs((pd.to_datetime(pnrs['ID_PNR_ITI_DEP_YMD_2'])-pd.to_datetime(pnrs['ID_PNR_ITI_DEP_YMD_1'])).apply(lambda x: x.days))\n",
    "# is pnr create hour in workhours or not\n",
    "pnrs['pnr_workhour_interval'] = pnrs['PNR_TIME'].apply(pnr_workhour_interval)\n",
    "# number of flights in each pnr\n",
    "pnrs['num_of_flights'] = pnrs['ID_PNR_ITI_DEP_YMD_1'].isna()*-1 + pnrs['ID_PNR_ITI_DEP_YMD_2'].isna()*-1+2\n",
    "pnrs['date'] = pnrs['ID_PNR_CREATION_DTIME_GMT'].apply(lambda x: x.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-crown",
   "metadata": {},
   "source": [
    "# tickets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "superior-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_raw = pd.read_csv('TICKET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "neural-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = tickets_raw[tickets_raw['FLIGHT_REASON']=='NONE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incorporate-factor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a80f57b5315e>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tickets['DEP'] = tickets['OND'].apply(lambda x: x.split('-')[0])\n",
      "<ipython-input-9-a80f57b5315e>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tickets['ARR'] = tickets['OND'].apply(lambda x: x.split('-')[1])\n",
      "<ipython-input-9-a80f57b5315e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tickets['date'] = tickets['ID_PNR_CREATION_YMD'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d').strftime('%Y-%m-%d'))\n"
     ]
    }
   ],
   "source": [
    "tickets['DEP'] = tickets['OND'].apply(lambda x: x.split('-')[0])\n",
    "tickets['ARR'] = tickets['OND'].apply(lambda x: x.split('-')[1])\n",
    "tickets['date'] = tickets['ID_PNR_CREATION_YMD'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d').strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brilliant-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.merge(tickets, pnrs, on=['PNR_NO', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "invisible-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.index = tickets.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sunset-wallace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8911797, 24), (7249326, 33), (8911797, 55))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickets.shape, pnrs.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efficient-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['FLIGHT_REASON']\n",
    "\n",
    "categorical_str1 = [\n",
    "'ARR',\n",
    "'DEP',\n",
    "'ID_PNR_ITI_CABCLASS_1',\n",
    "'ID_PNR_ITI_SELCLASS_1',\n",
    "'JRNY_TYP',\n",
    "'NTNLT1' ] \n",
    "\n",
    "categorical_str2 = [\n",
    "'ID_PNR_ITI_CABCLASS_2',\n",
    "'ID_PNR_ITI_SELCLASS_2']\n",
    "\n",
    "categorical_str = categorical_str1 + categorical_str2\n",
    "\n",
    "categorical_int1 = [\n",
    "'arrival_interval_1',\n",
    "'flight_month_1',\n",
    "'flight_weekday_1',\n",
    "'pnr_create_month',\n",
    "'pnr_create_weekday',\n",
    "'pnr_workhour_interval']\n",
    "\n",
    "categorical_int2 = [\n",
    "'arrival_interval_2',\n",
    "'flight_month_2',\n",
    "'flight_weekday_2']\n",
    "\n",
    "categorical_int = categorical_int1 + categorical_int2\n",
    "\n",
    "numerical1 = [\n",
    "'CUST_AGE',\n",
    "'diff_pnr_firstflight', \n",
    "'flight_duration_1']\n",
    "\n",
    "numerical2 = [\n",
    "'diff_second_firstflight',\n",
    "'flight_duration_2']\n",
    "\n",
    "numerical = numerical1+numerical2\n",
    "\n",
    "passthrough = [\n",
    "'dep_nat_flag',\n",
    "'arr_nat_flag',\n",
    "'CHILD_FLG',\n",
    "'FAMILY_FLG',\n",
    "'INFANT_FLG',\n",
    "'PET_FLG',\n",
    "'POS_POC_SAME_FLG',\n",
    "'SAME_SRNAME_FLG',\n",
    "'SEAT_SELECT_FLG',\n",
    "'SPORT_FLG',\n",
    "'XBAG_FIRST_FLT_FLG',\n",
    "'XBAG_LAST_FLT_FLG',\n",
    "'XBAG_TWO_WAY_FLT_FLG',\n",
    "'PNR_PSSG_COUNT']\n",
    "\n",
    "drops = [\n",
    "'ID_PNR_CREATION_DTIME_GMT',\n",
    "'ID_PNR_CREATION_YMD',\n",
    "'ID_PNR_ITI_ARR_APT_1',\n",
    "'ID_PNR_ITI_ARR_APT_2',\n",
    "'ID_PNR_ITI_DEP_APT_1',\n",
    "'ID_PNR_ITI_DEP_APT_2',\n",
    "'ID_TKT_NO',\n",
    "'OND',\n",
    "'PNR_NO',\n",
    "'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "informational-review",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.0 GiB for an array with shape (8911797, 55, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8963be4612d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#read in data here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmsgn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.45\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\missingno\\missingno.py\u001b[0m in \u001b[0;36mmatrix\u001b[1;34m(df, filter, n, p, sort, figsize, width_ratios, color, fontsize, labels, sparkline, inline, freq, ax)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# z is the color-mask array, g is a NxNx3 matrix. Apply the z color-mask to set the RGB of each pixel.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.0 GiB for an array with shape (8911797, 55, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "#read in data here\n",
    "msgn.matrix(test_df,color=(0.45, 0.15, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "iata_country = pd.read_excel('airport_country.xls', index_col='Unnamed: 0')\n",
    "airport_country_pairs = dict(zip(iata_country['IATA'], iata_country['Country']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the customer's nationality same with its departure country\n",
    "test_df['dep_nat_flag'] = (test_df['DEP'].map(airport_country_pairs) == test_df['NTNLT1']).map({True:1, False:0})\n",
    "# is the customer's nationality same with its arrival country\n",
    "test_df['arr_nat_flag'] = (test_df['ARR'].map(airport_country_pairs) == test_df['NTNLT1']).map({True:1, False:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_undersampling(X, y):\n",
    "    \n",
    "    num_sample = y[y=='STUDENT'].shape[0]\n",
    "    index_1 = y[y=='BUSINESS'].sample(num_sample, random_state=3).index\n",
    "    index_2 = y[y=='LEISURE'].sample(num_sample, random_state=3).index\n",
    "    index_3 = y[y=='SECOND HOME'].sample(num_sample, random_state=3).index\n",
    "    index_4 = y[y=='STUDENT'].index\n",
    "    \n",
    "    all_indexes = list(index_1) + list(index_2) + list(index_3) + list(index_4)\n",
    "\n",
    "    X_ = X.loc[all_indexes]\n",
    "    y_ = y[all_indexes]\n",
    "    \n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_without_clf(clf_name, clf_object):\n",
    "    \n",
    "    numeric_transformer1 = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    numeric_transformer2 = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=0.0)),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_str_transformer1 = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('one-hot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    categorical_str_transformer2 = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='-')),\n",
    "        ('one-hot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    categorical_int_transformer1 = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=-99)),\n",
    "        ('one-hot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    categorical_int_transformer2 = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=-100)),\n",
    "        ('one-hot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "                                    transformers=[\n",
    "                                                  ('num1', numeric_transformer1, numerical1),\n",
    "                                                  ('num2', numeric_transformer2, numerical2),\n",
    "                                                  ('cat_str1', categorical_str_transformer1, categorical_str1),\n",
    "                                                  ('cat_str2', categorical_str_transformer2, categorical_str2),\n",
    "                                                  ('cat_int1', categorical_int_transformer1, categorical_int1),\n",
    "                                                  ('cat_int2', categorical_int_transformer2, categorical_int2)], \n",
    "                                    n_jobs = -1,\n",
    "                                    transformer_weights = None,\n",
    "                                    remainder='passthrough')\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('f_selector', SelectKBest(f_classif, k='all')),\n",
    "                          (clf_name, clf_object)], verbose=False)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-apparatus",
   "metadata": {},
   "source": [
    "best results obtained from training with below classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = create_pipeline_without_clf('lgb',LGBMClassifier(learning_rate=0.01,max_depth=-1, min_samples_split=2, num_leaves=20, n_estimators= 500))\n",
    "clf2 = create_pipeline_without_clf('svc',SVC(C= 10, degree= 2, gamma= 0.1, kernel= 'rbf'))\n",
    "clf3 = create_pipeline_without_clf('log', LogisticRegression(C= 1.0, penalty= 'l2', max_iter=500))\n",
    "clf4 = create_pipeline_without_clf('rfc',RandomForestClassifier(max_depth= None, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 500))\n",
    "\n",
    "estimators_list = [('lgb', clf1),('svc', clf2),('log', clf3),('rfc', clf4)]\n",
    "stacking = StackingClassifier(estimators=estimators_list,  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data is obtained with nb 1-1\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "y_train = train['FLIGHT_REASON']\n",
    "X_train = train[categorical_str+categorical_int+numerical+passthrough]\n",
    "\n",
    "X_train, y_train = random_undersampling(X_train, y_train)\n",
    "\n",
    "# test data is constructed with all the NONE values in the FLIGHT_REASON column\n",
    "# not dropping outliers or nulls\n",
    "X_test = test_df[categorical_str+categorical_int+numerical+passthrough]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking.fit(X_train, y_train)\n",
    "preds = stacking.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_raw.loc[X_test.index, 'FLIGHT_REASON'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_raw.to_csv('TICKET_RESULT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_results = pd.DataFrame(preds, index=X_test.index,columns=['FLIGHT_REASON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_results.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
